{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "import pickle as cPickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger('./log/KeywordsFinder.log')\n",
    "logger.setLevel(20)\n",
    "handler = logging.FileHandler(filename='./log/Taxonomy_Generation.log', mode='w', encoding='utf-8')\n",
    "# handler.setLevel(20)\n",
    "formatter = logging.Formatter('%(asctime)s : %(filename)s : %(lineno)d : %(levelname)s : %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Config:\n",
    "    def __init__(self):\n",
    "        self.config = dict()\n",
    "        with open('./config/config.txt') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        return self.config[name]\n",
    "\n",
    "config = _Config()\n",
    "global corpus_file_path,base_dir_path, records, stopwords_file, spacy_nlp_model_name, min_df, non_spacy_words_excel_file_path, best_fit_analysis_excel_file_path, lower_factor_vocab,higher_factor_vocab, max_iter_min, max_iter_max, max_iter_incr, final_taxonomy_file_path, timestamp, directory_path, best_fit_tryouts_folder_path, filter_ner\n",
    "\n",
    "corpus_file_path = config.__getattr__('corpus_file_path')\n",
    "base_dir_path = config.__getattr__('base_dir_path')\n",
    "records = config.__getattr__('records')\n",
    "stopwords_file_path = config.__getattr__('stopwords_file_path')\n",
    "spacy_nlp_model_name = config.__getattr__('spacy_nlp_model_name')\n",
    "min_df = config.__getattr__('min_df')\n",
    "filter_ner = config.__getattr__('filter_ner').split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(spacy_nlp_model_name)\n",
    "stop_words = stopwords.words('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "roman_characters = re.compile(u'^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$')\n",
    "alpha_characters = re.compile(u'[^a-z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords from file\n",
    "additional_stopwords = None\n",
    "with open(stopwords_file_path) as f:\n",
    "    additional_stopwords = f.readlines()\n",
    "additional_stopwords = [x.strip() for x in additional_stopwords]\n",
    "stop_words.extend(additional_stopwords)\n",
    "stop_words = list(set(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load News data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of news - 5\n",
      "No. of sentences - 64\n",
      "No. of words - 2132\n"
     ]
    }
   ],
   "source": [
    "_dataset = pd.read_excel(corpus_file_path, nrows = records)\n",
    "#_dataset = pd.read_csv(corpus_file_path, nrows = records)\n",
    "\n",
    "logger.info(\"Number of Records - {}\".format(len(_dataset)))\n",
    "_dataset = _dataset.drop(['Unnamed: 0'], axis=1)\n",
    "_dataset['TEXT'] = _dataset[u'article_news_title']+ '. '+_dataset[u'article_news_abstract'] + '. '+_dataset[u'article_news_body']\n",
    "_dataset = _dataset[['id','TEXT']]\n",
    "_dataset['TEXT'] = _dataset.TEXT.apply(str)\n",
    "\n",
    "_dataset['sentences'] = _dataset['TEXT'].apply(sent_tokenize)\n",
    "\n",
    "news = _dataset.sentences.tolist()\n",
    "sentences = list(itertools.chain.from_iterable(news))\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.extend(sentence.split())\n",
    "\n",
    "print(\"No. of news - {}\".format(len(news)))\n",
    "print(\"No. of sentences - {}\".format(len(sentences)))\n",
    "print(\"No. of words - {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Noun Chunks extracted - 599\n",
      "Extracted_Noun_Chunks.xlsx\n"
     ]
    }
   ],
   "source": [
    "_dataset['noun_chunks'] = _dataset['sentences'].apply(lambda x: list(itertools.chain.from_iterable(map(lambda y: list(nlp(y).noun_chunks),x))))\n",
    "chnks = list(itertools.chain.from_iterable(_dataset['noun_chunks']))\n",
    "\n",
    "print(\"No. of Noun Chunks extracted - {}\".format(len(chnks)))\n",
    "_dataset.to_excel(\"Extracted_Noun_Chunks.xlsx\")\n",
    "print(\"Extracted_Noun_Chunks.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bring words to root form\n",
    "#### 2. Remove stop words\n",
    "#### 3. Remove Roman numbers\n",
    "#### 4. Remove words which are <= 2 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Noun chunks after cleaning - 503\n",
      "Cleaned_Noun_Chunks.xlsx\n"
     ]
    }
   ],
   "source": [
    "def is_roman(word):\n",
    "    if roman_characters.search(word.upper()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def cleaning_text(msg):\n",
    "    msg = str(msg).lower()\n",
    "    msg = alpha_characters.sub(' ', msg)\n",
    "    msg = [lemmatized_word for lemmatized_word in\n",
    "               [wordnet_lemmatizer.lemmatize(word, 'n') for word in msg.split() \n",
    "                if (word not in stop_words) and (not is_roman(word))\n",
    "               ] \n",
    "           if (lemmatized_word not in stop_words)\n",
    "           and (not is_roman(lemmatized_word))\n",
    "           and (len(lemmatized_word) > 2) \n",
    "          ]\n",
    "    msg=' '.join(list(filter(bool,msg)))\n",
    "    return msg\n",
    "\n",
    "def cleaning_list_of_text(list_of_msg):\n",
    "    return list(cleaning_text(x) for x in list_of_msg if len(cleaning_text(x)) > 0)\n",
    "\n",
    "_dataset['noun_chunks_cleaned'] = _dataset['noun_chunks'].apply(cleaning_list_of_text)\n",
    "_dataset.to_excel(\"Cleaned_Noun_Chunks.xlsx\")\n",
    "\n",
    "cleaned_chunks = _dataset.noun_chunks_cleaned.tolist()\n",
    "cleaned_chunks = list((itertools.chain.from_iterable(cleaned_chunks)))\n",
    "\n",
    "print(\"No. of Noun chunks after cleaning - {}\".format(len(cleaned_chunks)))\n",
    "print(\"Cleaned_Noun_Chunks.xlsx\")\n",
    "logger.info(\"No. of cleaned Noun Phrases - {}\".format(len(cleaned_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Noun chunks after N-gram filter - 263\n",
      "No. of unique Noun chunks after N-gram filter - 202\n"
     ]
    }
   ],
   "source": [
    "filtered_cleaned_chunks = filter(lambda x: len(x.split()) in [2,3,4] or (len(x.split()) == 1 and len(x) > 8), cleaned_chunks)\n",
    "term_2_count_map = Counter(filtered_cleaned_chunks)\n",
    "\n",
    "print(\"No. of Noun chunks after N-gram filter - {}\".format(sum(term_2_count_map.values())))\n",
    "print(\"No. of unique Noun chunks after N-gram filter - {}\".format(len(term_2_count_map.keys())))\n",
    "\n",
    "tf = pd.DataFrame({'term_phrase': list(term_2_count_map.keys()), 'count':list(term_2_count_map.values())})\n",
    "logger.info(\"Initial Noun Phrase corpus size: {}\".format(tf.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Noun chunks after TF filter - 12\n"
     ]
    }
   ],
   "source": [
    "tf = tf[tf['count'] >= min_df]\n",
    "\n",
    "print(\"No. of Noun chunks after TF filter - {}\".format(tf.shape[0]))\n",
    "logger.info(\"Noun Phrase corpus size after min_df filter({0}) : {1}\".format(3, tf.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Noun chunks after NER filter - 11\n",
      "Some of the chunks filtered  ['republican']\n"
     ]
    }
   ],
   "source": [
    "def is_ner_to_be_removed(input_text):\n",
    "    ner_doc = nlp(input_text)\n",
    "    for X in ner_doc.ents:\n",
    "        ner_tag = str(X.label_)\n",
    "        word = str(X.text)\n",
    "        if ner_tag in filter_ner:\n",
    "            return True\n",
    "        \n",
    "tf['is_ner_to_be_removed'] = tf['term_phrase'].apply(is_ner_to_be_removed)\n",
    "examples = tf[tf['is_ner_to_be_removed'] == True]['term_phrase'].tolist()[0:10]\n",
    "\n",
    "tf = tf[tf['is_ner_to_be_removed'] != True]\n",
    "\n",
    "print(\"No. of Noun chunks after NER filter - {}\".format(tf.shape[0]))\n",
    "print(\"Some of the chunks filtered \", examples)\n",
    "#tf.to_excel(\"tf.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS based filtering\n",
    "\n",
    "Remove Noun chunks which does not contain any noun term. \n",
    "\n",
    "They are least likely to form any topic.\n",
    "\n",
    "For ex. \"additional resource\", \"executive action\", \"local medium\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data frame with terms and their POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          term  NOUN  PROPN\n",
      "0    president   6.0    9.0\n",
      "1  immigration  16.0    NaN\n",
      "2   havenstein   NaN    4.0\n",
      "pos_df.xlsx\n"
     ]
    }
   ],
   "source": [
    "temp_term_list = np.empty([0,2])\n",
    "filtered_term_list = tf['term_phrase'].tolist()\n",
    "documents = _dataset.TEXT.tolist()\n",
    "for document in documents:\n",
    "    _list = []\n",
    "    doc = nlp(document)\n",
    "    for t in doc:\n",
    "        lemma_text = wordnet_lemmatizer.lemmatize(t.text.lower(), 'n')\n",
    "        if (lemma_text not in stop_words) and (not is_roman(lemma_text)) and (len(lemma_text) > 2) and (t.text.lower() in filtered_term_list):\n",
    "            _list.append([lemma_text, t.pos_])#,t.tag_])\n",
    "    if len(_list) > 0:\n",
    "        temp_term_list = np.vstack([temp_term_list,_list])\n",
    "temp_term_list\n",
    "term_df = pd.DataFrame(data=temp_term_list, columns=['term','pos']) #,'tag'])\n",
    "logger.info(\"Tokens and corresponding POS tags Dataframe shape - {}\".format(term_df.shape))\n",
    "logger.info(term_df.head())\n",
    "#term_df.to_excel(\"term_df_1.xlsx\")\n",
    "\n",
    "unique_terms = list(term_df.term.unique())\n",
    "list_of_term_dict = []\n",
    "for term in unique_terms:\n",
    "    word_df = term_df[term_df.term == term]\n",
    "    word_df = word_df.groupby(by=['pos'],).count()\n",
    "    _dict = {pos_tag: count.term for pos_tag, count in word_df.iterrows()}\n",
    "    _dict['term'] = term\n",
    "    list_of_term_dict.append(_dict)\n",
    "# list_of_term_dict[:5]\n",
    "\n",
    "pos_df = pd.DataFrame.from_dict(list_of_term_dict)\n",
    "# re-ordering columns\n",
    "cols = list(pos_df)\n",
    "cols.insert(0, cols.pop(cols.index('term')))\n",
    "pos_df = pos_df.loc[:, cols]\n",
    "\n",
    "logger.info(\"\\n Tokens and corresponding POS tags with frequenct Dataframe shape - {}\".format(pos_df.shape))\n",
    "pos_df.to_excel(\"pos_df.xlsx\")\n",
    "print(pos_df.head())\n",
    "print(\"pos_df.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to check if term is noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_pos(term):\n",
    "\n",
    "    df = pos_df[pos_df.term == term].drop(['term'], axis=1).T\n",
    "    if df.shape[1] == 1:\n",
    "        df.columns = ['count']\n",
    "        df = df.fillna(0).astype(int).sort_values(by=['count'], ascending=False)\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def is_term_noun_in_corpus(term):\n",
    "    noun_set = set(['NOUN','PROPN'])\n",
    "    df = get_term_pos(term)\n",
    "    if df is not None:\n",
    "        top_term_pos = df[df['count'] != 0].head(1)['count'] # consider only highest pos\n",
    "        term_pos_set = set(top_term_pos.index.tolist()) \n",
    "        return True if len(term_pos_set.intersection(noun_set)) > 0 else False\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def is_phrase_term_noun_in_corpus(phrase):\n",
    "    terms = phrase.split()\n",
    "    terms = map(is_term_noun_in_corpus, terms)\n",
    "    return any(terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove keywords based on POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf['is_noun'] = tf['term_phrase'].apply(is_phrase_term_noun_in_corpus)\n",
    "tf = tf[tf['is_noun'] == True]\n",
    "\n",
    "# Sort the results based on statistics\n",
    "tf = tf.sort_values(['count'], ascending=[0])\n",
    "tf = tf[['term_phrase','count']].reset_index()\n",
    "tf = tf.drop(['index'], axis=1)\n",
    "\n",
    "tf.to_excel(\"keywords.xlsx\")\n",
    "logger.info(\"Noun Phrase corpus size after noun in chunk filter - {}\".format(tf.shape[0]))\n",
    "print(\"No. of Noun chunks after POS filter - {}\".format(tf.shape[0]))\n",
    "print(\"keywords.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
